{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Misc. Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\William\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\William\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\William\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('conll2000')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing a Chunked Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP This/DT)\n",
      "  (VP has/VBZ increased/VBN)\n",
      "  (NP the/DT risk/NN)\n",
      "  (PP of/IN)\n",
      "  (NP the/DT government/NN)\n",
      "  (VP being/VBG forced/VBN to/TO increase/VB)\n",
      "  (NP base/NN rates/NNS)\n",
      "  (PP to/TO)\n",
      "  (NP 16/CD %/NN)\n",
      "  (PP from/IN)\n",
      "  (NP their/PRP$ current/JJ 15/CD %/NN level/NN)\n",
      "  (VP to/TO defend/VB)\n",
      "  (NP the/DT pound/NN)\n",
      "  ,/,\n",
      "  (NP economists/NNS)\n",
      "  and/CC\n",
      "  (NP foreign/JJ exchange/NN market/NN analysts/NNS)\n",
      "  (VP say/VBP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "\n",
    "chunked_sentence = conll2000.chunked_sents()[3]\n",
    "print(chunked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training A Chunker\n",
    "## POS Tagging Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    " \n",
    "shuffled_conll_sents = list(conll2000.chunked_sents())\n",
    "random.shuffle(shuffled_conll_sents)\n",
    "train_sents = shuffled_conll_sents[:int(len(shuffled_conll_sents) * 0.9)]\n",
    "test_sents = shuffled_conll_sents[int(len(shuffled_conll_sents) * 0.9 + 1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.6%%\n",
      "    Precision:     80.9%%\n",
      "    Recall:        84.1%%\n",
      "    F-Measure:     82.5%%\n"
     ]
    }
   ],
   "source": [
    "from nltk import ChunkParserI, TrigramTagger\n",
    " \n",
    " \n",
    "class TrigramChunkParser(ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        # Extract only the (POS-TAG, IOB-CHUNK-TAG) pairs\n",
    "        train_data = [[(pos_tag, chunk_tag) for word, pos_tag, chunk_tag in tree2conlltags(sent)] \n",
    "                      for sent in train_sents]\n",
    " \n",
    "        # Train a TrigramTagger\n",
    "        self.tagger = TrigramTagger(train_data)\n",
    " \n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for word, pos in sentence]\n",
    " \n",
    "        # Get the Chunk tags\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    " \n",
    "        # Assemble the (word, pos, chunk) triplets\n",
    "        conlltags = [(word, pos_tag, chunk_tag) \n",
    "                     for ((word, pos_tag), (pos_tag, chunk_tag)) in zip(sentence, tagged_pos_tags)]\n",
    " \n",
    "        # Transform to tree\n",
    "        return conlltags2tree(conlltags)\n",
    " \n",
    " \n",
    "trigram_chunker = TrigramChunkParser(train_sents)\n",
    "print(trigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.2%%\n",
      "    Precision:     87.8%%\n",
      "    Recall:        91.1%%\n",
      "    F-Measure:     89.5%%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Iterable\n",
    "from nltk import ChunkParserI, ClassifierBasedTagger\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    " \n",
    "def features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('__START2__', '__START2__'), ('__START1__', '__START1__')] + list(tokens) + [('__END1__', '__END1__'), ('__END2__', '__END2__')]\n",
    "    history = ['__START2__', '__START1__'] + list(history)\n",
    " \n",
    "    # shift the index with 2, to accommodate the padding\n",
    "    index += 2\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "        'pos': pos,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'prev-word': prevword,\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    "    }\n",
    " \n",
    " \n",
    "class ClassifierChunkParser(ChunkParserI):\n",
    "    def __init__(self, chunked_sents, **kwargs):\n",
    "        assert isinstance(chunked_sents, Iterable)\n",
    " \n",
    "        # Transform the trees in IOB annotated sentences [(word, pos, chunk), ...]\n",
    "        chunked_sents = [tree2conlltags(sent) for sent in chunked_sents]\n",
    " \n",
    "        # Transform the triplets in pairs, make it compatible with the tagger interface [((word, pos), chunk), ...]\n",
    "        def triplets2tagged_pairs(iob_sent):\n",
    "            return [((word, pos), chunk) for word, pos, chunk in iob_sent]\n",
    "        chunked_sents = [triplets2tagged_pairs(sent) for sent in chunked_sents]\n",
    " \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=chunked_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    " \n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    " \n",
    "        # Transform the result from [((w1, t1), iob1), ...] \n",
    "        # to the preferred list of triplets format [(w1, t1, iob1), ...]\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    " \n",
    "        # Transform the list of triplets to nltk.Tree format\n",
    "        return conlltags2tree(iob_triplets)\n",
    " \n",
    "classifier_chunker = ClassifierChunkParser(train_sents)\n",
    "print (classifier_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Hello/NNP world/NN)\n",
      "  !/.\n",
      "  (NP My/PRP$ name/NN)\n",
      "  (VP is/VBZ)\n",
      "  (NP William/NNP)\n",
      "  and/CC\n",
      "  (NP I/PRP)\n",
      "  (VP am/VBP typing/VBG)\n",
      "  (PP on/IN)\n",
      "  (NP the/DT computer/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    " \n",
    "# Something from today's NYTimes paper:\n",
    "print (classifier_chunker.parse(pos_tag(word_tokenize(\"Hello world! My name is William and I am typing on the computer.\"))))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hello\"\n",
      "\"    \"\n",
      "\"World\"\n",
      "\"!\"\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp('Hello     World!')\n",
    "for token in doc:\n",
    "    print('\"' + token.text + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Street Journal NP Journal\n",
      "an interesting piece NP piece\n",
      "crypto currencies NP currencies\n",
      "\n",
      "\n",
      "My name NP name\n",
      "William NP William\n",
      "I NP I\n",
      "the computer NP computer\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "doc2 = nlp(\"My name is William and I am typing on the computer.\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.label_, chunk.root.text)\n",
    "    \n",
    "print(\"\\n\")\n",
    "    \n",
    "for chunk in doc2.noun_chunks:\n",
    "    print(chunk.text, chunk.label_, chunk.root.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Wall Street Journal NP Wall\n",
      "una interesante pieza NP pieza\n",
      "monedas NP monedas\n",
      "\n",
      "\n",
      "Me NP Me\n",
      "William NP William\n",
      "yo NP yo\n",
      "la computadora NP computadora\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"El Wall Street Journal acaba de publicar una interesante pieza sobre monedas criptogr√°ficas\")\n",
    "doc2 = nlp(\"Me llamo es William, y yo estoy tecleando en la computadora.\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.label_, chunk.root.text)\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "for chunk in doc2.noun_chunks:\n",
    "    print(chunk.text, chunk.label_, chunk.root.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-018ef4e51039>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'The author is writing a new book.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'<VERB>?<ADV>*<VERB>+'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mlists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_regex_matches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'textacy' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
